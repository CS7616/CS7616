{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 4 | Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Course:** CS7616  \n",
    "**Due:**  Fri, Apr 29, 11:55pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "---\n",
    "This are just the source links for technical reference. Please go to the T-square resources page to download the pre-processed data ready for you to use in this assessment.\n",
    "\n",
    "| Dataset | Description | Data |\n",
    "|---|---|---|\n",
    "| MNIST | [[Description]](https://www.kaggle.com/c/digit-recognizer) | [[Data]](see_Tsquare) |\n",
    "| Sunset | [[Description]](see_Tsquare) | [[Data]](see_Tsquare) |\n",
    "| Office | [[Description]](see_Tsquare) | [[Data]](see_Tsquare) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST [20 Points]\n",
    "\n",
    "---\n",
    "For MNIST, you must build your own multiple layer convolutional neural network (at least two convolutional layers). Your general network architecture might be something like the following:\n",
    "\n",
    "Conv1 -> Activation -> Pooling -> Conv2 -> Activation -> Pooling -> Linear -> Activation -> Linear -> Softmax\n",
    "\n",
    "You will want to experiment with different convolutional kernel sizes, different activation types, dropout, and different architectures. \n",
    "You must do the following:\n",
    "* Train two different architectures (from scratch) and report results (confusion matrix) (i.e. with and without dropout, with more convolutional layers, kernels, etc).\n",
    "* Visualize the learned convolutional kernels from the two (or more) convolutional layers you use.\n",
    "* In your writeup, write the gradient descent equation used with the parameters you used to train your network. Write the forward pass equation for just your last layer (linear layer) and the backward pass (gradient equations).\n",
    "\n",
    "For reference on MNIST, you may want to look at the following:\n",
    "* Caffe - http://caffe.berkeleyvision.org/gathered/examples/mnist.html\n",
    "* Tensorflow - https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network\n",
    "* Torch - https://github.com/torch/demos/blob/master/train-a-digit-classifier/train-on-mnist.lua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunset [40 Points]\n",
    "\n",
    "---\n",
    "With the MNIST dataset, the number of training images was quite large, more than suffecent to learn an acruate model with CNNs. However, such may not always be the case. \n",
    "\n",
    "Lets say your working for a hot new tech-startup, called CnapShat™. You have the clever idea of using scene understanding to determine the best kind of hipster-filter™ to sugest users apply before sharing online. For example, if a sunset is recognized in the photo, latest trends recomend users might want to apply a warm vintage filter, while if somthing like a dark ally is recognized, then a gritty cinmatic felter might be better. Not to burst your bubble, but your colleague, Bob, tells you Google Photos' Auto Awesome sort of does this already. But you don't care, and procede anyhow. However, Bob was careless and lost most of the cloud database in a migration snafu, leaving only few remaing photos on a old server forgoten in a broom closet. Users are revolting having lost their posts, Bob was fired yesterday, and you boss walks in with a small usb-stick, tasking you to train a CNN clasifier for the new hail-mary-feature that will save CnapShat™ from bankruptcy.\n",
    "\n",
    "You don't really have enough samples from the remaining dataset, nor the computing time to train a CNN sunset detector from scratch, so you decide to bootstrap yourself by starting from a pretrained network such as GoogLeNet or AlexNet. This gets you close:\n",
    "\n",
    "| GoogLeNet | AlexNet |\n",
    "|:--:|:--:|\n",
    "|!['Sunset Model - GoogLeNet.png'](Sunset Model - GoogLeNet.png) | !['Sunset Model - AlexNet.png'](Sunset Model - AlexNet.png)|\n",
    "|!['Sunset Model - GoogLeNet - Time.png'](Sunset Model - GoogLeNet - Time.png) | !['Sunset Model - AlexNet - Time.png'](Sunset Model - AlexNet - Time.png)|\n",
    "\n",
    "Although it still is sort of hit or miss:\n",
    "\n",
    "| GoogLeNet |\n",
    "|:--:|\n",
    "|![Hit!](fig1.png)|\n",
    "|![Miss](fig2.png)|\n",
    "\n",
    "| True Positive | False Negative |\n",
    "|:--:|:--:|\n",
    "|![Hit!](fig4.jpg)|![Miss](fig3.jpg)|\n",
    "\n",
    "\n",
    "But CnapShat™'s future is on the line here! And you'd like to release something better. You happen to stumble across a **relevent** paper from 2003 [1], and you think that image recomposition might just help.\n",
    "\n",
    "* Finetune an architecture of your choice for this problem. This shouldn't take too long even on a CPU, so we expect good results.\n",
    "* Read [2] if you haven't already.\n",
    "* Explore using some sort of image recomposition/data augmentation (like [1] or [2]) to bolster you small dataset and report your results (confusion matrix) and observations. This could be changing colorspace representation, adding crops or hue to generate more augmented data, or something else to augment data.\n",
    "* What false positive or false negative images can you find from the test set or the web that still trip up your network and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Office [40 Points]\n",
    "\n",
    "---\n",
    "\n",
    "The office data is more difficult because now we are looking at pixel level classification instead of image level classification. We are also going to simplify it and ignore temporal connections. Given the architectures above, how do we change them to do pixel level classification?\n",
    "\n",
    "![OfficeNet!](office.png)\n",
    "\n",
    "Some ways you might go about it are below:\n",
    "\n",
    "This can be seen as in architecture here: https://ethereon.github.io/netscope/#/gist/3f2c75f3c8c71357f24c\n",
    "\n",
    "In this architecture, we replace the former linear layers with convolutional layers, this is called net surgery in caffe [3] (Hint we can copy the weights as well for better initialization). \n",
    "\n",
    "---\n",
    "   \n",
    "This can be seen as an architecture here: https://ethereon.github.io/netscope/#/gist/21529b597b2cf89f0ff22d52b3ac15e3\n",
    "\n",
    "In this architecture, we still do classification, but instead of the whole image, we do it on segments. This is done by segmenting the image beforehand, and then using Region-of-Interest pooling to pool over the activations from each segmented region to generate classifications.\n",
    "\n",
    "--- \n",
    "\n",
    "You could also try a convolutional encoder-decoder architecture, or any other architecture you might think of.\n",
    "We will release caffe models and prototxts of the above architectures trained on slightly different data for you to finetune on. \n",
    "They will be here:\n",
    "https://gist.github.com/StevenHickson/21529b597b2cf89f0ff22d52b3ac15e3\n",
    "\n",
    "You may want to use my caffe branch since it has all the changes required to do both FCN and Segmentation/ROI pooling as described above. It can be found here:  \n",
    "[Ubuntu](https://github.com/StevenHickson/caffe)  \n",
    "[Windows](https://github.com/StevenHickson/caffe-windows/tree/segLayer)  \n",
    "\n",
    "Torch can also load caffe modules if you wish to use that.\n",
    "\n",
    "You must do the following:\n",
    "* Finetune an architecture of your choice for this problem. This might take a while on the CPU but it is actually not too bad to only do 1-2 epochs which is the minimum of what we expect.\n",
    "* Generate a confusion matrix of your results (This MUST be colored and easy to see, do not give us a list of numbers).\n",
    "* Discuss what your network was good/bad at. Hypothesize why it might have failed or excelled in certain scenarios.\n",
    "\n",
    "## Extra Credit [25 points]\n",
    "* Beat our quickly created (only RGB) results on the same data (you might have good luck doing this with data augmentation, deeper architectures, or changing parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Submit\n",
    "\n",
    "---\n",
    "Submit a your code and a PDF report with results in a zip file. You can use any programming language. The code should be easily understandable. Please include a README.md file that describes how the TAs are to run your code.\n",
    "\n",
    "We expect the report to contain all of the following parts. The Report should contain the following:\n",
    "\n",
    "* All of the requirements listed in the sections above.\n",
    "* You must include your code (minus the caffe library. If you have specific code changes to libraries or forks of libraries, include a link to github) as well to get credit. Make sure to include your network code either via torch files, or prototxts, etc. No code submitted means a zero in the assignment.\n",
    "* If you are using a library from somewhere else, please mention it here as well. We hope you use Piazza for this so more people benefit.\n",
    "* You are encouraged to discuss and help others with anything short of giving them your code. There are many references on-line, especially with MNIST. However, you MAY NOT use final models directly from the Internet (you can fine-tune models though). You must train the models yourselves. Due to the way initialization works with models, we will know if you submit a preexisting model as a final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "[1] M. Bautell, J. Luo and R. T. Gray, *\"Sunset scene classification using simulated image recomposition,\"* Multimedia and Expo, 2003. ICME '03. Proceedings. 2003 International Conference on, 2003, pp. I-37-40 vol.1.\n",
    "doi: 10.1109/ICME.2003.1220848\n",
    "[ieeexplore](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1220848&isnumber=27433)\n",
    "[PDF](https://www.rose-hulman.edu/Users/faculty/young/CS-Classes/csse463/201020/Papers/boutell-icme03.pdf)  \n",
    "[2] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n",
    "[PDF](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)  \n",
    "[3] https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
