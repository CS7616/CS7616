{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 1 | Linear Clasification and Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Course:** CS7616  \n",
    "**Due:**  Sun, Feb 7, 11:55pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem\n",
    "\n",
    "---\n",
    "In the lectures we discussed linear classification and use of subspaces. Now we are going to write and train a set of classifiers and apply it to datasets below. We also discussed normal distributions and you are going to learn normal distributions from training data through Parametric Density Estimation and apply it to the rest.\n",
    "\n",
    "* You must write PCA, LDA, and a Naive Bayes linear classifier. You will use PCA for data compression and LDA as a linear classifier. You are allowed to use functions that do Eigen decomposition and the SVD but you MAY NOT use any functions that implement PCA, LDA, or linear classification for you. You may use all basic vector and matrix math functions.\n",
    "\n",
    "For this Problem Set you would be required to do the following for each of the two datasets listed below. For these classifications, you will be doing binary training and testing instead of training on all classes to simplify the classification problem.\n",
    "\n",
    "#### For Wine Dataset\n",
    "\n",
    "1. Select class 1 and 2. Then train and test on those for all of the following tests to make this classification binary.\n",
    "* Take a training set of **five** randomly chosen examples of each class and test on the rest of the\n",
    "data.\n",
    "* Take a training set of **fifty** randomly chosen examples of each class and test on the rest of the\n",
    "data.\n",
    "* Do **10-fold** cross-validation on the complete given data.\n",
    "\n",
    "#### For MNIST Dataset\n",
    "1. Use the test/train split provided by us in the train.csv and test.csv. You may wish to take a subset of the training data for validation as it is improper to tune your parameters on the test set. \n",
    "* Train and test on just the numbers of 0 and 1.\n",
    "* Train and test on just the numbers of 3 and 5.\n",
    "\n",
    "The csv files are [785 , NumSamples] where the image is 28x28 stacked in a 784 length vector followed by the class label. You will need to seperate 0,1 samples and 3,5 sampless for your training and testing.\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "\n",
    "For Bayesian Estimation, we are going to build a classifier by comparing the posterior probability for every class. For a class $C_k$ and data sample $x$, we have\n",
    "\n",
    "$$ p(C_k|x) = \\dfrac{p(x|C_k)p(C_k)}{p(x)} $$\n",
    "\n",
    "For this problem set we are going to assume a normal form for the class conditional.\n",
    "\n",
    "$$ p(x|C_k) = \\mathcal{N}(x|\\mu_k,\\Sigma_k) $$\n",
    "\n",
    "Lets say, \n",
    "\n",
    "$$ \\theta = (\\mu_k, \\Sigma_k) $$\n",
    "\n",
    "Given a set of training data D the parameter estimation problem entails finding the \"best\" parameter $\\theta$ given D. The method for Parameter Estimation, that we’ll be using is:  \n",
    "\n",
    "#### **Bayesian** Estimation (BE) - estimate the distribution $p(\\theta|D)$\n",
    "\n",
    "For **Naive Bayes**, we make a conditional independence assumption that reduces the complexity and number of parameters. To simply even further, we are going to assume that our distribution is gaussian and look at a binary classification case. With this, our probability simplifies to the following:\n",
    "\n",
    "$$ p(C_1|x) = 1 - p(C_0|x) = \\dfrac{exp(w_0 + \\sum_{i=1}^{n} w_i x_i)}{1 + exp(w_0 + \\sum_{i=1}^{n} w_i x_i)} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ w_0 = ln(\\dfrac{1 - \\pi}{\\pi}) + \\sum_{i}^{n} \\dfrac{ \\mu_{i1}^2 - \\mu_{i0}^2}{2 \\sigma_i^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ w_i = \\dfrac{ \\mu_{i1}^2 - \\mu_{i0}^2}{\\sigma_i^2} $$ \n",
    "\n",
    "Refer to slides and textbook for more details on these estimation techniques.\n",
    "What we did not discuss in class how to obtain the prior $p(C_k)$ without domain knowledge. One way in which you can do it for this PS is by estimating a normal distribution over the complete dataset irrespective of class. So, you might keep the prior on $C_k$ the same for every class. We leave it up to you to decide how to add a prior into this equation. You may also wish to read this:\n",
    "https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf\n",
    "\n",
    "#### Linear Discriminant Analysis (LDA) - Fit a linear classifier\n",
    "\n",
    "See the notes\n",
    "\n",
    "#### Principla Component Analysis (PCA) - Find the principal components (fit an ellipsoid)\n",
    "\n",
    "See the notes and review the EigenFace paper and you may want to reference:\n",
    "http://arxiv.org/pdf/1404.1100.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "---\n",
    "This are just the source links for technical reference. Please go to the **T-square resources** page to download the pre-processed data ready for you to use in this assessment.\n",
    "\n",
    "| Dataset | Description | Data |\n",
    "|---|---|---|\n",
    "| Wine Data Set | [[Description]](https://archive.ics.uci.edu/ml/datasets/Wine) | [[Data]](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/) |\n",
    "| MNIST | [[Description]](https://www.kaggle.com/c/digit-recognizer) | [[Data]](https://www.kaggle.com/c/digit-recognizer/data) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Submit\n",
    "\n",
    "---\n",
    "Submit a your code and a PDF report with results in a zip file. You can use any programming language, but we will prefer `Python` or `Matlab`. The code should be easily runnable. Please include a README.md file that describes how the TAs are to run your code.\n",
    "\n",
    "We expect the report to contain all of the following parts. The Report should contain the following:\n",
    "\n",
    "* A concise description of the methods you used that seemed to work. [5 points]\n",
    "* For PCA/LDA, display the most important eigenvector and the 20th eigenvector (show these as an image for MNIST). [15 points]\n",
    "* For PCA/LDA, plot the sorted cumulative sum of eigenvalues (divide each one by the total). Pick how many eigenvalues to retain for your compression. Explain how you picked that number (you may want to reference the former graph).  [15 points]\n",
    "* For PCA/LDA, reconstruct a test example and show the reconstruction error. For MNIST, visualize this reconstruction and the error.  [5 points]\n",
    "* For LDA/NB, A confusion matrix on the results of your algorithm on the test data.  [15 points]\n",
    "* For LDA/NB, An accuracy table that has all of your classification results  [15 points]\n",
    "* A short description of the results.  [5 points]\n",
    "* A succinct explanation of the why you think you achieved those results. Hypothesize why certain techniques or cases worked out better than the other. If you have data/plots to support your claim, even better.  [10 points]\n",
    "*  For 0,1 and 3,5 MNIST test cases, discuss whatever difference you might have in accuracy, explain why you think this might be. [10 points]\n",
    "* This classification task was binary, explain what you might try for more classes using these same methods. [5 points]\n",
    "\n",
    "* You must include your code as well to get credit. No code submitted means a zero in the assignment.\n",
    "* If you are using a library from somewhere else, please mention it here as well. We hope you use Piazza for this so more people benefit.\n",
    "* You are encouraged to discuss and help others with anything short of giving them your code. There are many refrences online, especially with MNIST. However, you MAY NOT use code from the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Suggestions\n",
    "\n",
    "---\n",
    "Although it hasn’t been discussed yet, data pre-processing forms an integral part of performing pattern recognition. We will discuss here a few easy to perform techniques that help. Some features may not be very useful in helping determine the class, they may be uncorrelated, or linearly dependent on others, or just random noise. So, you might like to try to all possible subsets of the set of available features. Another technique, \"whitening\", normalizes each of the features by subtracting each feature(dimension) by the mean and divide by the standard deviation. It is useful when your features are at very difficult scales. As a caveat, these are only suggestions, you don’t have to try them and they might not even help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
